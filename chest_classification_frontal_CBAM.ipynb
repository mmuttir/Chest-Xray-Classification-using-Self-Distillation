{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a1dcb6f",
   "metadata": {},
   "source": [
    "# Chest Xray Abnormality Classification using Xception-Net and CBAM\n",
    "\n",
    "Loading required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,2\"\n",
    "# from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "from tensorflow.keras.layers import Conv3DTranspose as Deconvolution3D\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Conv3D, ZeroPadding3D, UpSampling3D, Dense, concatenate, Conv3DTranspose, Cropping3D, PReLU\n",
    "from tensorflow.keras.layers import MaxPooling3D, GlobalAveragePooling3D, AvgPool3D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import Input\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, CSVLogger, TensorBoard\n",
    "\n",
    "from sys import getsizeof\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "IMSIZE = 320,320\n",
    "RANDOM_SEED = 1234\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1234)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(1234)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import glob as glob\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import json\n",
    "import SimpleITK as sitk\n",
    "from glob import glob \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import savemat\n",
    "\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure, feature\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import imutils\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from skimage import data, img_as_float\n",
    "from skimage import exposure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0424bdc7",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "1- Normalization\n",
    "2- Resizing the image\n",
    "3- Histogram Equalization to improve the contrast of chest images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm(scan):\n",
    "    m=np.mean(scan)\n",
    "    st=np.std(scan)\n",
    "    scan=(scan-m)/st\n",
    "#     scan = scan.astype(np.float64)\n",
    "    return scan\n",
    "\n",
    "def res_scan(nscan):\n",
    "#     W,H  =  shape[0],shape[1]\n",
    "#     desired_depth = D\n",
    "    desired_width = 320\n",
    "    desired_height = 320\n",
    "#     current_depth = nscan.shape[0]\n",
    "    current_width = nscan.shape[0]\n",
    "    current_height = nscan.shape[1]\n",
    "#     depth_factor = desired_depth/current_depth\n",
    "    width_factor = desired_width/current_width\n",
    "    height_factor = desired_height/current_height\n",
    "    nscan = ndimage.zoom(nscan,  ( width_factor,height_factor), order=1)\n",
    "    return nscan\n",
    "\n",
    "def preprocess_image(sc):\n",
    "    n = img_as_float(sc)\n",
    "    n = exposure.equalize_adapthist(n/np.max(n))\n",
    "    n=res_scan(n)\n",
    "    n=get_norm(n)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_samples(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data=shuffle(data)\n",
    "    data = data[['Path', 'No Finding']]\n",
    "    file_names = list(data.iloc[:,0])\n",
    "    # Get the labels present in the second column\n",
    "    labels = list(data.iloc[:,1])\n",
    "    samples=[]\n",
    "    for samp,lab in zip(file_names,labels):\n",
    "        samples.append([samp,lab])\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b14df7b8",
   "metadata": {},
   "source": [
    "## Datagenerator\n",
    "\n",
    "Data generator allows to load one batch of images at a time thus it omits the need of loading all the images at the same time and do not occupy large RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_IDs,to_fit=True, batch_size=32, dim=(8 ,512, 512), shuffle=True):\n",
    "        \"\"\"Initialization\n",
    "        :param list_IDs: list of all 'label' ids to use in the generator\n",
    "        :param labels: list of image labels (file names)\n",
    "        :param image_path: path to images location\n",
    "        :param mask_path: path to masks location\n",
    "        :param to_fit: True to return X and y, False to return X only\n",
    "        :param batch_size: batch size at each iteration\n",
    "        :param dim: tuple indicating image dimension\n",
    "        :param n_channels: number of image channels\n",
    "        :param n_classes: number of output masks\n",
    "        :param shuffle: True to shuffle label indexes after every epoch\n",
    "        \"\"\"\n",
    "        self.list_IDs = list_IDs\n",
    "        self.to_fit = to_fit\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "\n",
    "        if self.to_fit:\n",
    "            X,y = self._generate_Xy(list_IDs_temp)\n",
    "            return X, y\n",
    "        else:\n",
    "            X = self._generate_X(list_IDs_temp)\n",
    "            return X\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "            \n",
    "    def _generate_Xy(self, list_IDs_temp):\n",
    "        \"\"\"Generates data containing batch_size images\n",
    "        :param list_IDs_temp: list of label ids to load\n",
    "        :return: batch of images\n",
    "        \"\"\"\n",
    "#         global filt\n",
    "\n",
    "            # Initialise X_train and y_train arrays for this batch\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            img_name = ID[0]\n",
    "            label = int(ID[1])\n",
    "            img =  Image.open(img_name)\n",
    "\n",
    "            # apply any kind of preprocessing\n",
    "            img = img_as_float(img)\n",
    "            img = exposure.equalize_adapthist(img/np.max(img))\n",
    "            img=res_scan(img)\n",
    "            img=get_norm(img)\n",
    "            # Add example to arrays\n",
    "            X_train.append(img)\n",
    "            y_train.append(label)\n",
    "        X_train = np.array(X_train)\n",
    "        y_train = np.array(y_train)\n",
    "        y_train = np.asarray(y_train).reshape((-1,1))\n",
    "#         y_test = np.asarray(test_labels).astype('float32').reshape((-1,1))\n",
    "        return X_train,y_train\n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "    def _generate_y(self, list_IDs_temp):\n",
    "        \"\"\"Generates data containing batch_size masks\n",
    "        :param list_IDs_temp: list of label ids to load\n",
    "        :return: batch if masks\n",
    "        \"\"\"\n",
    "        # Initialization\n",
    "        y = np.empty((self.batch_size, *self.dim,1))\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            y[i,] = self._load_clip_y(ID)\n",
    "        return y\n",
    "    \n",
    "    def _load_clip_y(self, videopath):\n",
    "        \"\"\"Load grayscale image\n",
    "        :param image_path: path to image to load\n",
    "        :return: loaded image\n",
    "        \"\"\"\n",
    "        global output1_actual\n",
    "        temp = videopath.split('--')\n",
    "        frm = int(temp[2])\n",
    "        clip = np.array([output1_actual[frm:frm+8,:,:,:]])#frames[frm:frm+5,:,:,:]\n",
    "        return clip\n",
    "    \n",
    "\n",
    "    def _load_clip(self, videopath):\n",
    "        \"\"\"Load grayscale image\n",
    "        :param image_path: path to image to load\n",
    "        :return: loaded image\n",
    "        \"\"\"\n",
    "        global inputs\n",
    "        temp = videopath.split('--')\n",
    "        frm = int(temp[2])\n",
    "        clip = np.array([inputs[frm:frm+8,:,:,:]])#frames[frm:frm+5,:,:,:]\n",
    "        clip = clip + abs(np.min(clip))\n",
    "        clip = clip/(np.max(clip))\n",
    "        return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = load_samples(\"/home/amal/Chest Classification/index/train_frt.csv\")\n",
    "validation_samples = load_samples(\"/home/amal/Chest Classification/index/valid_frt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/amal/Chest Classification/index/valid_frt.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = DataGenerator(list_IDs=train_samples,to_fit=True, batch_size=12, dim=(320, 320), shuffle=True)\n",
    "Valid_datagen = DataGenerator(list_IDs=validation_samples,to_fit=True, batch_size=12, dim=(320, 320), shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04782632",
   "metadata": {},
   "source": [
    "## Model Defining and Compiling\n",
    "\n",
    "The Xception-Net is chosen for this problem along with CBAM module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import keras\n",
    "import glob\n",
    "\n",
    "from keras.utils import np_utils\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, ZeroPadding2D, GlobalAveragePooling2D, concatenate\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "IMSIZE = 320,320\n",
    "RANDOM_SEED = 1234\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1234)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalMaxPooling2D\n",
    "base_model = Xception(weights=None, include_top=False)\n",
    "\n",
    "\n",
    "input_tensor = Input(shape=(320, 320, 1))\n",
    "ip = Conv2D(3,(3,3),padding='same')(input_tensor)  \n",
    "x = base_model(ip)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "model = Model(inputs=input_tensor, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, Conv2D, Add, Activation, Lambda\n",
    "from keras import backend as K\n",
    "from keras.activations import sigmoid\n",
    "\n",
    "def attach_attention_module(net, attention_module):\n",
    "    if attention_module == 'cbam_block': # CBAM_block\n",
    "        net = cbam_block(net)\n",
    "    else:\n",
    "        raise Exception(\"'{}' is not supported attention module!\".format(attention_module))\n",
    "\n",
    "    return net\n",
    "\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "\t\"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
    "\tAs described in https://arxiv.org/abs/1807.06521.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tcbam_feature = channel_attention(cbam_feature, ratio)\n",
    "\tcbam_feature = spatial_attention(cbam_feature)\n",
    "\treturn cbam_feature\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "\t\n",
    "\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\tchannel = input_feature.shape[channel_axis]\n",
    "\t\n",
    "\tshared_layer_one = Dense(channel//ratio,\n",
    "\t\t\t\t\t\t\t activation='relu',\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\tshared_layer_two = Dense(channel,\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\t\n",
    "\tavg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "\tavg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\tavg_pool = shared_layer_one(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tavg_pool = shared_layer_two(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\t\n",
    "\tmax_pool = GlobalMaxPooling2D()(input_feature)\n",
    "\tmax_pool = Reshape((1,1,channel))(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\tmax_pool = shared_layer_one(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tmax_pool = shared_layer_two(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\t\n",
    "\tcbam_feature = Add()([avg_pool,max_pool])\n",
    "\tcbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\t\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\t\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "\tkernel_size = 7\n",
    "\t\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tchannel = input_feature.shape[1]\n",
    "\t\tcbam_feature = Permute((2,3,1))(input_feature)\n",
    "\telse:\n",
    "\t\tchannel = input_feature.shape[-1]\n",
    "\t\tcbam_feature = input_feature\n",
    "\t\n",
    "\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert avg_pool.shape[-1] == 1\n",
    "\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert max_pool.shape[-1] == 1\n",
    "\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "\tassert concat.shape[-1] == 2\n",
    "\tcbam_feature = Conv2D(filters = 1,\n",
    "\t\t\t\t\tkernel_size=kernel_size,\n",
    "\t\t\t\t\tstrides=1,\n",
    "\t\t\t\t\tpadding='same',\n",
    "\t\t\t\t\tactivation='sigmoid',\n",
    "\t\t\t\t\tkernel_initializer='he_normal',\n",
    "\t\t\t\t\tuse_bias=False)(concat)\t\n",
    "\tassert cbam_feature.shape[-1] == 1\n",
    "\t\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\t\t\n",
    "\treturn multiply([input_feature, cbam_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "#from attention_module import attach_attention_module\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=4, attention_module=None):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            # attention_module\n",
    "            if attention_module is not None:\n",
    "                y = attach_attention_module(y, attention_module)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_module = 'cbam_block'\n",
    "depth = 50 # For ResNet, specify the depth (e.g. ResNet50: depth=50)\n",
    "input_shape=320,320,1\n",
    "model = resnet_v1(input_shape=input_shape, depth=depth, attention_module=attention_module)\n",
    "adam = optimizers.Adam(lr=0.001, decay=1e-5)\n",
    "model.compile(optimizer=adam, loss=\"sparse_categorical_crossentropy\",metrics=[\"sparse_categorical_accuracy\"])\n",
    "log_dir = '/home/azka/chestclassification/'\n",
    "checkpoint_filepath=\"/home/azka/CBAM_frt.h5\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "# class_weights={0:1,1:2,2:3,3:5}\n",
    "batch_size=12\n",
    "stopping=tf.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy',\n",
    "                                          mode='max',\n",
    "                                          patience=30)\n",
    "train_steps = (len(train_samples)//batch_size)\n",
    "valid_steps = (len(validation_samples)//batch_size)\n",
    "history=model.fit(train_datagen,validation_data=Valid_datagen,epochs=50,batch_size=12,\n",
    "                  steps_per_epoch=train_steps,\n",
    "                  validation_steps=valid_steps,\n",
    "                  callbacks=[model_checkpoint_callback])#,class_weight=class_weights)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f32090b7",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"CBAM_actual_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mongo3",
   "language": "python",
   "name": "mongo3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
